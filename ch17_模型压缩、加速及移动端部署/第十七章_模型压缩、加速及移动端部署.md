[TOC]



# 第十七章 模型压缩及移动端部署

    Markdown Revision 1;
    Date: 2018/11/4
    Editor: 谈继勇
    Contact: scutjy2015@163.com
    updata:贵州大学硕士张达峰

## 17.1 为什么需要模型压缩和加速？
（1）对于在线学习和增量学习等实时应用而言，如何减少含有大量层级及结点的大型神经网络所需要的内存和计算量显得极为重要。  
（2）智能设备的流行提供了内存、CPU、能耗和宽带等资源，使得深度学习模型部署在智能移动设备上变得可行。   
（3）高效的深度学习方法可以有效的帮助嵌入式设备、分布式系统完成复杂工作，在移动端部署深度学习有很重要的意义。   
https://blog.csdn.net/Touch_Dream/article/details/78441332


## 17.2 目前有哪些深度学习模型压缩方法？
https://blog.csdn.net/wspba/article/details/75671573
https://blog.csdn.net/Touch_Dream/article/details/78441332

### 17.2.1 前端压缩
（1）知识蒸馏（简单介绍）  
一个复杂的模型可以认为是由多个简单模型或者强约束条件训练而来，具有很好的性能，但是参数量很大，计算效率低，而小模型计算效率高，但是其性能较差。知识蒸馏是让复杂模型学习到的知识迁移到小模型当中,使其保持其快速的计算速度前提下，同时拥有复杂模型的性能，达到模型压缩的目的。但与剪枝、量化等方法想比，效果较差。(https://blog.csdn.net/Lucifer_zzq/article/details/79489248)    
（2）紧凑的模型结构设计（简单介绍）   
紧凑的模型结构设计主要是对神经网络卷积的方式进行改进，比如使用两个3x3的卷积替换一个5x5的卷积、使用深度可分离卷积等等方式降低计算参数量。  
（3）滤波器层面的剪枝（简单介绍）   
参考链接  https://blog.csdn.net/JNingWei/article/details/79218745 补充优缺点  
滤波器层面的剪枝属于非结构花剪枝，主要是对较小的权重矩阵整个剔除，然后对整个神经网络进行微调。此方式由于剪枝过于粗放，容易导致精度损失较大，而且部分权重矩阵中会存留一些较小的权重造成冗余，剪枝不彻底。  
### 17.2.2 后端压缩  
（1）低秩近似   （简单介绍，参考链接补充优缺点）  
在卷积神经网络中，卷积运算都是以矩阵相乘的方式进行。对于复杂网络，权重矩阵往往非常大，非常消耗存储和计算资源。低秩近似就是用若干个低秩矩阵组合重构大的权重矩阵，以此降低存储和计算资源消耗。  
优点：  

* 可以降低存储和计算消耗；

* 一般可以压缩2-3倍；精度几乎没有损失；  

缺点：  

* 模型越复杂，权重矩阵越大，利用低秩近似重构参数矩阵不能保证模型的性能      

（2）未加限制的剪枝   （简单介绍，参考链接补充优缺点）     
剪枝操作包括：非结构化剪枝和结构化剪枝。非结构化剪枝是对神经网络中权重较小的权重或者权重矩阵进剔除，然后对整个神经网络进行微调；结构化剪枝是在网络优化目标中加入权重稀疏正则项，使部分权重在训练时趋于0。  

优点： 

* 保持模型性能不损失的情况下，减少参数量9-11倍；

* 剔除不重要的权重，可以加快计算速度，同时也可以提高模型的泛化能力；


缺点：  

* 非结构化剪枝会增加内存访问成本；

* 极度依赖专门的运行库和特殊的运行平台，不具有通用性；

* 压缩率过大时，破坏性能；     

（3）参数量化   （简单介绍，参考链接补充优缺点）   
神经网络的参数类型一般是32位浮点型，使用较小的精度代替32位所表示的精度。或者是将多个权重映射到同一数值，权重共享  
优点：

* 模型性能损失很小，大小减少8-16倍；

缺点：

* 压缩率大时，性能显著下降；

* 依赖专门的运行库，通用性较差；    

（4）二值网络  （简单介绍，参考链接补充优缺点）    
对于32bit浮点型数用1bit二进制数-1或者1表示。  
优点：

* 网络体积小，运算速度快     


目前深度学习模型压缩方法的研究主要可以分为以下几个方向： 
（1）更精细模型的设计。目前很多网络基于模块化设计思想，在深度和宽度两个维度上都很大，导致参数冗余。因此有很多关于模型设计的研究，如SqueezeNet、MobileNet等，使用更加细致、高效的模型设计，能够很大程度的减少模型尺寸，并且也具有不错的性能。    
（2）模型裁剪。结构复杂的网络具有非常好的性能，其参数也存在冗余，因此对于已训练好的模型网络，可以寻找一种有效的评判手段，将不重要的connection或者filter进行裁剪来减少模型的冗余。    
（3）核的稀疏化。在训练过程中，对权重的更新进行诱导，使其更加稀疏，对于稀疏矩阵，可以使用更加紧致的存储方式，如CSC，但是使用稀疏矩阵操作在硬件平台上运算效率不高，容易受到带宽的影响，因此加速并不明显。   
（4）量化    
（5）Low-rank分解   
（6）迁移学习   


## 17.3 目前有哪些深度学习模型优化加速方法？
https://blog.csdn.net/nature553863/article/details/81083955
模型优化加速能够提升网络的计算效率，具体包括：   
（1）Op-level的快速算法：FFT Conv2d (7x7, 9x9), Winograd Conv2d (3x3, 5x5) 等；   
（2）Layer-level的快速算法：Sparse-block net [1] 等；   
（3）优化工具与库：TensorRT (Nvidia), Tensor Comprehension (Facebook) 和 Distiller (Intel) 等；   

原文：https://blog.csdn.net/nature553863/article/details/81083955   

## 17.4 影响神经网络速度的4个因素（再稍微详细一点）
1. FLOPs(FLOPs就是网络执行了多少multiply-adds操作)；  
2. MAC(内存访问成本)；   
3. 并行度(如果网络并行度高，速度明显提升)；   
4. 计算平台(GPU，ARM)   

## 17.5 改变网络结构设计为什么会实现模型压缩、加速？
### 1. Group convolution
Group convolution最早出现在AlexNet中，是为了解决单卡显存不够，将网络部署到多卡上进行训练。Group convolution可以减少单个卷积1/g的参数量。  
假设输入特征的的维度为H \* W \* c1；卷积核的维度为h1 \* w1 \* c1，共c2个；输出特征的维度为 H1 \* W1 \* c2。  
传统卷积计算方式如下：  
![image](./img/ch17/1.png)
传统卷积运算量为：  
```math
A = H * W * h1 * w1 * c1 * c2
```
Group convolution是将输入特征的维度c1分成g份，每个group对应的channel数为c1/g，特征维度H \* W \* c1/g；，每个group对应的卷积核的维度也相应发生改变为h1 \* w1 \* c1/9，共c2/g个；每个group相互独立运算，最后将结果叠加在一起。  
Group convolution计算方式如下：  
![image](./img/ch17/2.png)
Group convolution运算量为：  

```math
B = H * W * h1 * w1 * c1/g * c2/g * g
```
Group卷积相对于传统卷积的运算量：  
```math
\dfrac{B}{A} = \dfrac{ H * W * h1 * w1 * c1/g * c2/g * g}{H * W * h1 * w1 * c1 * c2} = \dfrac{1}{g}
```
由此可知：group卷积相对于传统卷积减少了1/g的参数量。

### 2. Depthwise separable convolution
Depthwise separable convolution是由depthwise conv和pointwise conv构成。  
depthwise conv(DW)有效减少参数数量并提升运算速度。但是由于每个feature map只被一个卷积核卷积，因此经过DW输出的feature map不能只包含输入特征图的全部信息，而且特征之间的信息不能进行交流，导致“信息流通不畅”。  
pointwise conv(PW)实现通道特征信息交流，解决DW卷积导致“信息流通不畅”的问题。
假设输入特征的的维度为H \* W \* c1；卷积核的维度为h1 \* w1 \* c1，共c2个；输出特征的维度为 H1 \* W1 \* c2。  
传统卷积计算方式如下：  
![image](./img/ch17/3.png)
传统卷积运算量为：  
```math
A = H * W * h1 * w1 * c1 * c2
```
DW卷积的计算方式如下：  
![image](./img/ch17/4.png)
DW卷积运算量为： 
```math
B_DW = H * W * h1 * w1 * 1 * c1
```
PW卷积的计算方式如下：
![image](./img/ch17/5.png)
```math
B_PW = H_m * W_m * 1 * 1 * c1 * c2
```
Depthwise separable convolution运算量为：
```math
B = B_DW + B_PW  
```
Depthwise separable convolution相对于传统卷积的运算量：
```math
\dfrac{B}{A} = \dfrac{ H * W * h1 * w1 * 1 * c1 + H_m * W_m * 1 * 1 * c1 * c2}{H * W * h1 * w1 * c1 * c2}  

= \dfrac{1}{c2} + \dfrac{1}{h1 * w1}
```
由此可知，随着卷积通道数的增加，Depthwise separable convolution的运算量相对于传统卷积更少。

### 3. 输入输出的channel相同时，MAC最小
**卷积层的输入和输出特征通道数相等时MAC最小，此时模型速度最快。**  
假设feature map的大小为h*w，输入通道c1，输出通道c2。  
已知：
```math
FLOPs = B = h * w * c1 * c2

=> c1 * c2 = \dfrac{B}{h * w}

MAC = h * w * (c1 + c2) + c1 * c2

c1 + c2 \geq 2 * \sqrt{c1 * c2}

=> MAC \geq 2 * h * w \sqrt{\dfrac{B}{h * w}} + \dfrac{B}{h * w} 
```
根据均值不等式得到(c1-c2)^2>=0，等式成立的条件是c1=c2，也就是输入特征通道数和输出特征通道数相等时，在给定FLOPs前提下，MAC达到取值的下界。

### 4. 减少组卷积的数量
**过多的group操作会增大MAC，从而使模型速度变慢**  
由以上公式可知，group卷积想比与传统的卷积可以降低计算量，提高模型的效率；如果在相同的FLOPs时，group卷积为了满足FLOPs会是使用更多channels，可以提高模型的精度。但是随着channel数量的增加，也会增加MAC。  
FLOPs：
```math
B = \dfrac{h * w * c1 * c2}{g}
```
MAC：
```math
MAC = h * w * (c1 + c2) + \dfrac{c1 * c2}{g}
```
由MAC，FLOPs可知：
```math
MAC = h * w * c1 + \dfrac{B*g}{c1} + \dfrac{B}{h * w}
```
当FLOPs固定(B不变)时，g越大，MAC越大。

### 5. 减少网络碎片化程度(分支数量)
**模型中分支数量越少，模型速度越快**  
此结论主要是由实验结果所得。  
以下为网络分支数和各分支包含的卷积数目对神经网络速度的影响。  
![image](./img/ch17/6.png)
实验中使用的基本网络结构，分别将它们重复10次，然后进行实验。实验结果如下：
![image](./img/ch17/7.png)
由实验结果可知，随着网络分支数量的增加，神经网络的速度在降低。网络碎片化程度对GPU的影响效果明显，对CPU不明显，但是网络速度同样在降低。

### 6. 减少元素级操作
**元素级操作所带来的时间消耗也不能忽视**  
ReLU ，Tensor 相加，Bias相加的操作，分离卷积（depthwise convolution）都定义为元素级操作。  
FLOPs大多数是对于卷积计算而言的，因为元素级操作的FLOPs相对要低很多。但是过的元素级操作也会带来时间成本。ShuffleNet作者对ShuffleNet v1和MobileNet v2的几种层操作的时间消耗做了分析，发现元素级操作对于网络速度的影响也很大。
![image](./img/ch17/8.png)

## 17.6 常用的轻量级网络有哪些？（再琢磨下语言和排版）
* **SqueezeNet**
* **MobileNet**
* **ShuffleNet**
* **Xception**

### 1. SequeezeNet
SqueenzeNet出自F. N. Iandola, S.Han等人发表的论文《SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and < 0.5MB model size》，作者在保证精度不损失的同时，将原始AlexNet压缩至原来的510倍。  
#### 1.1 设计思想
在网络结构设计方面主要采取以下三种方式：
* 用1\*1卷积核替换3\*3卷积
    * 理论上一个1\*1卷积核的参数是一个3\*3卷积核的1/9，可以将模型尺寸压缩9倍。
* 减小3\*3卷积的输入通道数
    * 根据上述公式，减少输入通道数不仅可以减少卷积的运算量，而且输入通道数与输出通道数相同时还可以减少MAC。
* 延迟降采样
    * 分辨率越大的输入能够提供更多特征的信息，有利于网络的训练判断，延迟降采样可以提高网络精度。
#### 1.2 网络架构
SqueezeNet提出一种多分支结构——fire model，其中是由Squeeze层和expand层构成。Squeeze层是由s1个1\*1卷积组成，主要是通过1\*1的卷积降低expand层的输入维度；expand层利用e1个1\*1和e3个3\*3卷积构成多分支结构提取输入特征，以此提高网络的精度(其中e1=e3=4*s1)。
![image](./img/ch17/9.png)
SqueezeNet整体网络结构如下图所示：
![image](./img/ch17/10.png)
#### 1.3实验结果
不同压缩方法在ImageNet上的对比实验结果
![image](./img/ch17/11.png)
由实验结果可知，SqueezeNet不仅保证了精度，而且将原始AlexNet从240M压缩至4.8M，压缩50倍，说明此轻量级网络设计是可行。

### 2. MobileNet
MobileNet 是Google团队于CVPR-2017的论文《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications》中针对手机等嵌入式设备提出的一种轻量级的深层神经网络，该网络结构在VGG的基础上使用DW+PW的组合，在保证不损失太大精度的同时，降低模型参数量。
#### 2.1 设计思想
* 采用深度可分离卷积代替传统卷积
    * 采用DW卷积在减少参数数量的同时提升运算速度。但是由于每个feature map只被一个卷积核卷积，因此经过DW输出的feature map不能只包含输入特征图的全部信息，而且特征之间的信息不能进行交流，导致“信息流通不畅”。
    * 采用PW卷积实现通道特征信息交流，解决DW卷积导致“信息流通不畅”的问题。
* 使用stride=2的卷积替换pooling
    * 直接在卷积时利用stride=2完成了下采样，从而节省了需要再去用pooling再去进行一次下采样的时间，可以提升运算速度。同时，因为pooling之前需要一个stride=1的 conv，而与stride=2 conv的计算量想比要高近4倍(**个人理解**)。
#### 2.2 网络架构
* DW conv和PW conv
MobileNet的网络架构主要是由DW conv和PW conv组成，相比于传统卷积可以降低`$\dfrac{1}{N} + \dfrac{1}{Dk}$`倍的计算量。  
标准卷积与DW conv和PW conv如图所示:  
![image](./img/ch17/12.png)
深度可分离卷积与传统卷积运算量对比：
![image](./img/ch17/13.png)
网络结构：
![image](./img/ch17/14.png)

* MobileNets的架构
![image](./img/ch17/15.png)

#### 2.3 实验结果
![image](./img/ch17/16.png)
由上表可知，使用相同的结构，深度可分离卷积虽然准确率降低1%，但是参数量减少了6/7。

### 3. MobileNet-v2
MobileNet-V2是2018年1月公开在arXiv上论文《Inverted Residuals and Linear Bottlenecks: Mobile Networks for Classification, Detection and Segmentation》，是对MobileNet-V1的改进，同样是一个轻量化卷积神经网络。
#### 3.1 设计思想
* 采用Inverted residuals
    * 为了保证网络可以提取更多的特征，在residual block中第一个1\*1 Conv和3*3 DW Conv之前进行通道扩充
* Linear bottlenecks
    * 为了避免Relu对特征的破坏，在residual block的Eltwise sum之前的那个 1\*1 Conv 不再采用Relu
* stride=2的conv不使用shot-cot，stride=1的conv使用shot-cut

#### 3.2 网络架构
*  Inverted residuals  
ResNet中Residuals block先经过1\*1的Conv layer，把feature map的通道数降下来，再经过3\*3 Conv layer，最后经过一个1\*1 的Conv layer，将feature map 通道数再“扩张”回去。即采用先压缩，后扩张的方式。而 inverted residuals采用先扩张，后压缩的方式。  
MobileNet采用DW conv提取特征，由于DW conv本身提取的特征数就少，再经过传统residuals block进行“压缩”，此时提取的特征数会更少，因此inverted residuals对其进行“扩张”，保证网络可以提取更多的特征。
![image](./img/ch17/17.png)
*  Linear bottlenecks  
ReLu激活函数会破坏特征。ReLu对于负的输入，输出全为0，而本来DW conv特征通道已经被“压缩”，再经过ReLu的话，又会损失一部分特征。采用Linear，目的是防止Relu破坏特征。
![image](./img/ch17/18.png)
* shortcut  
stride=2的conv不使用shot-cot，stride=1的conv使用shot-cut
![image](./img/ch17/19.png)
* 网络架构  
![image](./img/ch17/20.png)

### 4. Xception
Xception是Google提出的，arXiv 的V1 于2016年10月公开《Xception: Deep Learning with Depthwise Separable Convolutions 》，Xception是对Inception v3的另一种改进，主要是采用depthwise separable convolution来替换原来Inception v3中的卷积操作。
#### 4.1设计思想
* 采用depthwise separable convolution来替换原来Inception v3中的卷积操作  
    与原版的Depth-wise convolution有两个不同之处：
    * 第一个：原版Depth-wise convolution，先逐通道卷积，再1*1卷积; 而Xception是反过来，先1\*1卷积，再逐通道卷积；
    * 第二个：原版Depth-wise convolution的两个卷积之间是不带激活函数的，而Xception在经过1\*1卷积之后会带上一个Relu的非线性激活函数；

#### 4.2网络架构
feature map在空间和通道上具有一定的相关性，通过Inception模块和非线性激活函数实现通道之间的解耦。增多3\*3的卷积的分支的数量，使它与1\*1的卷积的输出通道数相等，此时每个3\*3的卷积只作用与一个通道的特征图上，作者称之为“极致的Inception（Extream Inception）”模块，这就是Xception的基本模块。
![image](./img/ch17/21.png)

### 5. ShuffleNet-v1
ShuffleNet 是Face++团队提出的，晚于MobileNet两个月在arXiv上公开《ShuffleNet： An Extremely Efficient Convolutional Neural Network for Mobile Devices 》用于移动端前向部署的网络架构。ShuffleNet基于MobileNet的group思想，将卷积操作限制到特定的输入通道。而与之不同的是，ShuffleNet将输入的group进行打散，从而保证每个卷积核的感受野能够分散到不同group的输入中，增加了模型的学习能力。
#### 5.1 设计思想
* 采用group conv减少大量参数
    * roup conv与DW conv存在相同的“信息流通不畅”问题 
* 采用channel shuffle解决上述问题
    * MobileNet中采用PW conv解决上述问题，SheffleNet中采用channel shuffle
* 采用concat替换add操作
    * avg pooling和DW conv(s=2)会减小feature map的分辨率，采用concat增加通道数从而弥补分辨率减小而带来信息的损失

#### 5.2 网络架构
MobileNet中1\*1卷积的操作占据了约95%的计算量，所以作者将1\*1也更改为group卷积，使得相比MobileNet的计算量大大减少。
![image](./img/ch17/22.png)
group卷积与DW存在同样使“通道信息交流不畅”的问题，MobileNet中采用PW conv解决上述问题，SheffleNet中采用channel shuffle。  
ShuffleNet的shuffle操作如图所示
![image](./img/ch17/24.png)
avg pooling和DW conv(s=2)会减小feature map的分辨率，采用concat增加通道数从而弥补分辨率减小而带来信息的损失；实验表明：多多使用通道(提升通道的使用率)，有助于提高小模型的准确率。
![image](./img/ch17/23.png)
网络结构：  
![image](./img/ch17/25.png)

### 6. ShuffleNet-v2
huffleNet-v2 是Face++团队提出的《ShuffleNet V2: Practical Guidelines for Ecient CNN Architecture Design》，旨在设计一个轻量级但是保证精度、速度的深度网络。
#### 6.1 设计思想
* 文中提出影响神经网络速度的4个因素：
    * a. FLOPs(FLOPs就是网络执行了多少multiply-adds操作)
    * b. MAC(内存访问成本)
    * c. 并行度(如果网络并行度高，速度明显提升)
    * d. 计算平台(GPU，ARM)
* ShuffleNet-v2 提出了4点网络结构设计策略：
    * G1.输入输出的channel相同时，MAC最小
    * G2.过度的组卷积会增加MAC
    * G3.网络碎片化会降低并行度
    * G4.元素级运算不可忽视  

#### 6.2 网络结构

depthwise convolution 和 瓶颈结构增加了 MAC，用了太多的 group，跨层连接中的 element-wise Add 操作也是可以优化的点。所以在 shuffleNet V2 中增加了几种新特性。  
所谓的 channel split 其实就是将通道数一分为2，化成两分支来代替原先的分组卷积结构（G2），并且每个分支中的卷积层都是保持输入输出通道数相同（G1），其中一个分支不采取任何操作减少基本单元数（G3），最后使用了 concat 代替原来的 elementy-wise add，并且后面不加 ReLU 直接（G4），再加入channle shuffle 来增加通道之间的信息交流。 对于下采样层，在这一层中对通道数进行翻倍。 在网络结构的最后，即平均值池化层前加入一层 1x1 的卷积层来进一步的混合特征。
![image](./img/ch17/26.png)
网络结构  
![image](./img/ch17/27.png)

#### 6.4  ShuffleNet-v2具有高精度的原因
* 由于高效，可以增加更多的channel，增加网络容量
* 采用split使得一部分特征直接与下面的block相连，特征复用(DenseNet)


## 17.7 现有移动端开源框架及其特点（介绍开源者及时间，特点、功能、优势）

### 17.7.1 NCNN   
ncnn （GitHub地址：https://github.com/Tencent/ncnn ）
https://cloud.tencent.com/developer/article/1005805

特点：      
​	NCNN其架构设计以手机端运行为主要原则，考虑了手机端的硬件和系统差异以及调用方式。无第三方依赖，跨平台，手机端 CPU 的速度快于目前所有已知的开源框架。基于 ncnn，开发者能够将深度学习算法轻松移植到手机端高效执行，开发出人工智能 APP，将 AI 带到你的指尖。ncnn 目前已在腾讯多款应用中使用，如 QQ，Qzone，微信，天天P图等。   

功能：    
1、NCNN支持卷积神经网络、多分支多输入的复杂网络结构，如主流的 vgg、googlenet、resnet、squeezenet 等。     
2、NCNN无需依赖任何第三方库。    
3、NCNN全部使用C/C++实现，以及跨平台的cmake编译系统，可轻松移植到其他系统和设备上。    
4、汇编级优化，计算速度极快。使用ARM NEON指令集实现卷积层，全连接层，池化层等大部分 CNN 关键层。   
5、精细的数据结构设计，内存占用极低。没有采用需构造出非常大的矩阵，消耗大量内存的通常框架——im2col + 矩阵乘法。   
6、支持多核并行计算，优化CPU调度。   
7、整体库体积小于500K，可精简到小于300K。   
8、可扩展的模型设计，支持8bit 量化和半精度浮点存储。   
9、支持直接内存引用加载网络模型。   
10、可注册自定义层实现并扩展。   

### 17.7.2 QNNPACK


特点：

Facebook开源高性能内核库QNNPACK
https://baijiahao.baidu.com/s?id=1615725346726413945&wfr=spider&for=pc
http://www.sohu.com/a/272158070_610300

### 

支持移动端深度学习的几种开源框架
https://blog.csdn.net/zchang81/article/details/74280019

### 17.7.3 mobile-deep-learning

百度开源移动端深度学习框架mobile-deep-learning  2017 年 9 月 25 日
https://yq.aliyun.com/ziliao/303275

移动端开源深度学习框架
https://blog.csdn.net/kazehouri/article/details/80672672

### 17.7.4 MACE


小米开源移动端深度学习框架MACE，自主研发，专为IoT设备优化
https://baijiahao.baidu.com/s?id=1604593002655309086&wfr=spider&for=pc

移动端深度学习框架小结
https://blog.csdn.net/yuanlulu/article/details/80857211?utm_source=blogxgwz7

### 17.7.5 PocketFlow
腾讯AI Lab开源模型压缩与加速框架PocketFlow
https://mp.weixin.qq.com/s?src=11&timestamp=1541335410&ver=1224&signature=JbHfKmuDNQ6E2cWFUu4*xLU0z4yXa3btbsqCrLpI20U8QAHX6-O*vIgwch159sclcBKb7qzAM4dro2S1n2Mk7a**KqXqTQSRi8i5xGWEIZbw7ZDIkpacRdxB0EHAX1Bo&new=1

### 17.7.6 其他几款支持移动端深度学习的开源框架
https://blog.csdn.net/zchang81/article/details/74280019


## 17.8 移动端开源框架部署

### 17.8.1 以NCNN为例
部署步骤   

### 17.8.2 以QNNPACK为例
部署步骤     

## 17.9 移动端开源框架部署疑难


