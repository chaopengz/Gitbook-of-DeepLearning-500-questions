[TOC]



# 第八章  目标检测

**目标检测负责人：**

- 稿定科技计算机视觉工程师-刘鹏
- 哈工大博士袁笛
- 上海大学硕士陈方杰：1609951733@qq.com

**TODO**

- [ ] 配图路径待修改

**目录**

8.1 基于候选区域的目标检测器

8.1.1 滑动窗口检测器

8.1.2 选择性搜索

8.1.3 R-CNN系列

8.1.3.1 R-CNN

8.1.3.2 Fast R-CNN

8.1.3.3 Faster R-CNN

8.1.3.4 R-CNN方法的性能

8.2 基于区域的全卷积神经网络（R-FCN）

8.3 单次目标检测器

8.3.1 SSD系列

8.3.1.1 SSD

8.3.1.2 DSSD

8.3.1.3 FSSD

8.3.2 YOLO系列

8.3.2.1 YOLOv1

8.3.2.2 YOLOv2

8.3.2.3 YOLO9000

8.3.2.4 YOLOv3

8.3.3 RetinaNet



## 8.1 基于候选区域的目标检测器

### 8.1.1 滑动窗口检测器

    自从AlexNet获得ILSVRC 2012挑战赛的冠军之后，采用卷积神经网络（Convolutional Neural Network, CNN）进行分类任务就逐渐成为了主流研究方向。最常见的一种采用暴力方法进行目标检测的思路是在图片上从左向右、从上至下通过滑动窗口，利用分类思想进行目标识别。为了能够在不同的观察距离检测不同的目标类型，通常使用不同大小和宽高比的滑动窗口。

![](./img/ch8/8.1.1.png)

     图8.1.1 滑动窗口示意图（从右向左，从上至下）
    
    采用滑动窗口从图像中剪切相应的图像块。由于很多分类器只能采用固定大小的图像块，因此这些图像块大多是原始图像块经过变形转换得到的。尽管如此，这样的操作并不影响分类的准确率，因为分类器可以处理变形后的图像块。

![](./img/ch8/8.1.2.png)

    图8.1.2 将图像转换成固定尺寸的图像
    
    经过变形后的图像块被输入到CNN分类器中，用来提取出4096维特征。之后，使用SVM分类器识别该图像的类别，并且采用线性回归输出该图像的边界框。

![](./img/ch8/8.1.3.png)

    图8.1.3 滑动窗口检测器的系统工作流程图

### 8.1.2 选择性搜索 

    除了暴力检测方法之外，选择性搜索也是一种常见的目标检测方法。采用候选区域方法（region proposal method）创建目标检测的感兴趣区域（ROI）。在选择性搜索（selective search，SS）中，假设现在图像上有n个预分割的区域,表示为R={R1, R2, ..., Rn},计算每个region与它相邻region(注意是相邻的区域)的相似度,这样会得到一个n*n的相似度矩阵(同一个区域之间和一个区域与不相邻区域之间的相似度可设为NaN),从矩阵中找出最大相似度值对应的两个区域,将这两个区域合二为一,这时候图像上还剩下n-1个区域;重复上面的过程(只需要计算新的区域与它相邻区域的新相似度,其他的不用重复计算),重复一次,区域的总数目就少1,知道最后所有的区域都合并称为了同一个区域(即此过程进行了n-1次,区域总数目最后变成了1).算法的流程图如下图所示:
![](./img/ch8/al8.1.png)    

    下图第一行展示了如何使区域增长，第二行中的蓝色矩形代表合并过程中所有可能的ROI。
![](./img/ch8/8.1.4.png)

    图8.1.4 选择性搜索
### 8.1.3 R-CNN系列 

#### 8.1.3.1 R-CNN

    从2014年CNN就成为用于目标检测的极热门的工具，CNN可以很好地帮我们完成图像识别的任务，我们只需要添加一些额外的功能来完成目标的定位任务即可。使用一个高容量的卷积神经网络将region proposals自底而上的传播，用来定位目标是很好的选择，如果有标签的训练数据比较少，可以使用训练好的参数作为辅助，进行fine-tuning，能够得到非常好的定位效果提升。R-CNN利用候选区域方法创建了约2000个ROI。这些区域被转换为固定大小的图像，并分别馈送到卷积神经网络中。该网络结构后面会跟有几个全连接层，以实现目标分类并提炼出目标的边界框。
![](./img/ch8/8.1.5.png)

    图8.1.5 使用候选区域、CNN、仿射层来定位目标
    使用RCNN进行目标检测，主要分三个部分：（1）产生不依赖与特定类别的region proposals，这些region proposals定义了一个整个检测器可以获得的候选目标；（2）一个大的卷积神经网络，对每个region产生一个固定长度的特征向量；（3）一系列特定类别的线性SVM分类器。以下是R-CNN整个系统的流程图：
![](./img/ch8/8.1.6.png)

    图8.1.6 采用R-CNN进行目标检测的流程图
    通过使用更少且更高质量的ROI，采用R-CNN能够比采用滑动窗口方法更快速、更准确地进行目标检测。

##### 边界框回归器

    采用候选区域方法进行目标检测具有非常高的计算复杂度。为了加速这个过程，通常会使用计算量较少的候选区域选择方法构建ROI，并在后面使用线性回归器（使用全连接层）进一步提炼边界框。
![](./img/ch8/8.1.7.png)

    图8.1.7 使用回归方法将蓝色的原始边界框提炼为红色

#### 8.1.3.2 Fast R-CNN

    R-CNN需要非常多的候选区域以提升目标检测的准确度，但其实有很多区域是彼此重叠的，因此使得R-CNN的训练和推断速度非常慢。如果我们有2000个候选区域，且每个候选区域都需要独立地馈送到CNN中，那么对于不同的ROI，我们需要重复提取2000次特征。此外，CNN中的特征图以一种密集的方式表征空间特征，是否能直接使用特征图代替原图来检测目标。
![](./img/ch8/8.1.8.png)

    图8.1.8 图像特征图
![](./img/ch8/8.1.9.png)

    图8.1.9 直接利用特征图计算ROI
    Fast R-CNN使用特征提取器（CNN）提取整个图像的特征，而不是从头开始对每个图像块提取多次。可以将创建候选区域的方法直接应用到提取到的特征图上。例如，Fast R-CNN选择VGG16中的卷积层Conv5来生成ROI，这些关注区域随后会结合对应的特征图以裁剪为特征图块，并用于目标检测任务中。我们使用ROI池化将特征图块转换为固定的大小，并馈送到全连接层进行分类和定位。因为Fast-RCNN不会重复提取特征，因此它能显著地减少处理时间。
![](./img/ch8/8.1.10.png)

    图8.1.10
    将候选区域直接应用于特征图，并使用ROI池化将其转化为固定大小的特征图块。以下是Fast R-CNN的流程图：
![](./img/ch8/8.1.11.png) 

    图8.1.11 Fast R-CNN流程图
    Fast R-CNN最重要的一点就是包含特征提取器、分类器和边界框回归器在内的整个网络能通过多任务损失函数进行端到端的训练，这种多任务损失即结合了分类损失和定位损失的方法，大大提升了模型准确度。

##### ROI池化

    因为Fast R-CNN使用全连接层，所以应用ROI池化将不同大小的ROI转换为固定大小。为简洁起见，我们先将8×8特征图转换为预定义的2×2大小。下图左上角：特征图。右上角：将ROI（蓝色区域）与特征图重叠。左下角：将ROI拆分为目标维度。例如，对于2×2目标，我们将ROI分割为4个大小相似或相等的部分。右下角：找到每个部分的最大值，得到变换后的特征图。
![](./img/ch8/8.1.12.png)

    图7.1.12 ROI池化
    输入特征图（左上），输出特征图（右下），ROI(右上，蓝色框)。按上述步骤得到一个2×2的特征图块，可以馈送至分类器和边界框的回归器中。

#### 8.1.3.3 Faster R-CNN  

    Fast R-CNN依赖于外部候选区域方法，如选择性搜索。但这些算法在CPU上运行且速度很慢。在测试中，Fast R-CNN需要2.3秒来进行预测，其中2秒用于生成2000个ROI。Faster R-CNN采用与Fast R-CNN相同的设计，只是它用内部深层网络代替了候选区域方法。新的候选区域网络（RPN）在生成ROI时效率更高，并且以每幅图像10毫秒的速度运行。
![](./img/ch8/8.1.13.png) 

    图8.1.13 Faster R-CNN的流程图
    Faster R-CNN的流程图与Fast R-CNN相同，采用外部候选区域方法代替了内部深层网络。
![](./img/ch8/8.1.14.png) 

    图8.1.14

##### 候选区域网络

    候选区域网络（RPN）将第一个卷积网络的输出特征图作为输入。它在特征图上滑动一个3×3的卷积核，以使用卷积网络（如下所示的ZF网络）构建与类别无关的候选区域。其他深度网络（如VGG或ResNet）可用于更全面的特征提取，但这需要以速度为代价。ZF网络最后会输出256个值，它们将馈送到两个独立的全连接层，以预测边界框和两个objectness分数，这两个objectness分数度量了边界框是否包含目标。我们其实可以使用回归器计算单个objectness分数，但为简洁起见，Faster R-CNN使用只有两个类别的分类器：即带有目标的类别和不带有目标的类别。
![](./img/ch8/8.1.15.png) 

    图8.1.15
    对于特征图中的每一个位置，RPN会做k次预测。因此，RPN将输出4×k个坐标和每个位置上2×k个得分。下图展示了8×8的特征图，且有一个3×3的卷积核执行运算，它最后输出8×8×3个ROI（其中k=3）。下图（右）展示了单个位置的3个候选区域。
![](./img/ch8/8.1.16.png)

    图8.1.16
    假设最好涵盖不同的形状和大小。因此，Faster R-CNN不会创建随机边界框。相反，它会预测一些与左上角名为锚点的参考框相关的偏移量（如x, y）。我们限制这些偏移量的值，因此我们的猜想仍然类似于锚点。
![](./img/ch8/8.1.17.png) 

    图8.1.17
    要对每个位置进行k个预测，我们需要以每个位置为中心的k个锚点。每个预测与特定锚点相关联，但不同位置共享相同形状的锚点。
![](./img/ch8/8.1.18.png) 

    图8.1.18
    这些锚点是精心挑选的，因此它们是多样的，且覆盖具有不同比例和宽高比的现实目标。这使得我们可以用更好的猜想来指导初始训练，并允许每个预测专门用于特定的形状。该策略使早期训练更加稳定和简便。
![](./img/ch8/8.1.19.png)

    图8.1.19
    Faster R-CNN使用更多的锚点。它部署9个锚点框：3个不同宽高比的3个不同大小的锚点框。每一个位置使用9个锚点，每个位置会生成2×9个objectness分数和4×9个坐标。

#### 8.1.3.4 R-CNN方法的性能 

    如下图所示，Faster R-CNN的速度要快得多。
![](./img/ch8/8.1.20.png)

    图8.1.20性能对比



    表8.1.1 性能对比

## 8.2 基于区域的全卷积神经网络（R-FCN）

    假设我们只有一个特征图用来检测右眼。那么我们可以使用它定位人脸吗？应该可以。因为右眼应该在人脸图像的左上角，所以我们可以利用这一点定位整个人脸。如果我们还有其他用来检测左眼、鼻子或嘴巴的特征图，那么我们可以将检测结果结合起来，更好地定位人脸。现在我们回顾一下所有问题。在Faster R-CNN中，检测器使用了多个全连接层进行预测。如果有2000个ROI，那么成本非常高。R-FCN通过减少每个ROI所需的工作量实现加速。上面基于区域的特征图与ROI是独立的，可以在每个ROI之外单独计算。剩下的工作就比较简单了，因此R-FCN的速度比Faster R-CNN快。
![](./img/ch8/8.2.1.png)

    图8.2.1 人脸检测
    现在我们来看一下5×5的特征图M，内部包含一个蓝色方块。我们将方块平均分成3×3个区域。现在，我们在M中创建了一个新的特征图，来检测方块的左上角（TL）。这个新的特征图如下图（右）所示。只有黄色的网格单元[2,2]处于激活状态。在左侧创建一个新的特征图，用于检测目标的左上角。
![](./img/ch8/8.2.2.png) 

    图8.2.2 检测示例
    我们将方块分成9个部分，由此创建了9个特征图，每个用来检测对应的目标区域。这些特征图叫做位置敏感得分图（position-sensitive score map），因为每个图检测目标的子区域（计算其得分）。
![](./img/ch8/8.2.3.png)

    图8.2.3生成9个得分图
    下图中红色虚线矩形是建议的ROI。我们将其分割成3×3个区域，并询问每个区域包含目标对应部分的概率是多少。例如，左上角ROI区域包含左眼的概率。我们将结果存储成3×3 vote数组，如下图（右）所示。例如，vote_array[0][0]包含左上角区域是否包含目标对应部分的得分。
![](./img/ch8/8.2.4.png) 

    图8.2.4
    将ROI应用到特征图上，输出一个3x3数组。将得分图和ROI映射到vote数组的过程叫做位置敏感ROI池化（position-sensitive ROI-pool）。该过程与前面讨论过的ROI池化非常接近。
![](./img/ch8/8.2.5.png) 

    图8.2.5
    将ROI的一部分叠加到对应的得分图上，计算V[i][j]。在计算出位置敏感ROI池化的所有值后，类别得分是其所有元素得分的平均值。
![](./img/ch8/8.2.6.png)

    图8.2.6 ROI池化
    假如我们有C个类别要检测。我们将其扩展为C+1个类别，这样就为背景（非目标）增加了一个新的类别。每个类别有3×3个得分图，因此一共有(C+1)×3×3个得分图。使用每个类别的得分图可以预测出该类别的类别得分。然后我们对这些得分应用 softmax 函数，计算出每个类别的概率。以下是数据流图，在本案例中，k=3。
![](./img/ch8/8.2.7.png) 

    图8.2.7

## 8.3 单次目标检测器

    我们将对单次目标检测器（包括SSD、YOLO、YOLOv2、YOLOv3）进行综述。我们将分析FPN以理解多尺度特征图如何提高准确率，特别是小目标的检测，其在单次检测器中的检测效果通常很差。然后我们将分析Focal loss和RetinaNet，看看它们是如何解决训练过程中的类别不平衡问题的。
### 8.3.1 SSD系列  

#### 8.3.1.1 SSD

```
Faster R-CNN中，在分类器之后有一个专用的候选区域网络。
```

![](./img/ch8/8.3.1.png)

```
图8.3.1 Faster R-CNN工作流
基于区域的检测器是很准确的，但需要付出代价。Faster R-CNN在PASCAL VOC 2007测试集上每秒处理7帧的图像（7FPS）。和R-FCN类似，研究者通过减少每个ROI的工作量来精简流程。
作为替代，我们是否需要一个分离的候选区域步骤？我们可以直接在一个步骤内得到边界框和类别吗？
```

```
让我们再看一下滑动窗口检测器。我们可以通过在特征图上滑动窗口来检测目标。对于不同的目标类型，我们使用不同的窗口类型。以前的滑动窗口方法的致命错误在于使用窗口作为最终的边界框，这就需要非常多的形状来覆盖大部分目标。更有效的方法是将窗口当作初始猜想，这样我们就得到了从当前滑动窗口同时预测类别和边界框的检测器。
```

![](./img/ch8/8.3.2.png)

```
图8.3.2
基于滑动窗口进行预测这个概念和Faster R-CNN中的锚点很相似。然而，单次检测器会同时预测边界框和类别。例如，我们有一个8×8特征图，并在每个位置做出k个预测，即总共有8×8×k个预测结果。
```

![](./img/ch8/8.3.3.png) 

```
图8.3.3 64个位置
在每个位置，我们有k个锚点（锚点是固定的初始边界框猜想），一个锚点对应一个特定位置。我们使用相同的锚点形状仔细地选择锚点和每个位置。
```

![](./img/ch8/8.3.4.png) 

```
图8.3.4 使用4个锚点在每个位置做出4个预测
以下是4个锚点（绿色）和4个对应预测（蓝色），每个预测对应一个特定锚点。在Faster R-CNN中，我们使用卷积核来做5个参数的预测：4个参数对应某个锚点的预测边框，1个参数对应objectness置信度得分。因此3×3×D×5卷积核将特征图从8×8×D转换8×8×5。
```

![](./img/ch8/8.3.5.png)

```
图8.3.5 4个预测，每个预测对应一个锚点
```

![](./img/ch8/8.3.6.png) 

```
图8.3.6 使用3×3卷积核计算预测
在单次检测器中，卷积核还预测C个类别概率以执行分类（每个概率对应一个类别）。因此我们应用一个3×3×D×25卷积核将特征图从8×8×D转换为8×8×25（C=20）。
```

![](./img/ch8/8.3.7.png) 

```
图8.3.7 每个位置做出k个预测，每个预测有25个参数
单次检测器通常需要在准确率和实时处理速度之间进行权衡。它们在检测太近距离或太小的目标时容易出现问题。
```

    SSD是使用VGG19网络作为特征提取器（和Faster R-CNN中使用的CNN一样）的单次检测器。我们在该网络之后添加自定义卷积层（蓝色），并使用卷积核（绿色）执行预测。
![](./img/ch8/8.3.8.png)

    图8.3.8 同时对类别和位置执行单次预测
    然而，卷积层降低了空间维度和分辨率。因此上述模型仅可以检测较大的目标。为了解决该问题，我们从多个特征图上执行独立的目标检测。
![](./img/ch8/8.3.9.png) 

    图8.3.9 使用多尺度特征图用于检测
#### 8.3.1.2 DSSD

TODO

#### 8.3.1.3 FSSD

TODO

### 8.3.2 YOLO系列

#### 8.3.2.1 YOLOv1
    YOLO之前的物体检测系统使用分类器来完成物体检测任务。为了检测一个物体，这些物体检测系统要在一张测试图的不同位置和不同尺寸的bounding box上使用该物体的分类器去评估是否有该物体。如DPM系统，要使用一个滑窗（sliding window）在整张图像上均匀滑动，用分类器评估是否有物体。
    在DPM之后提出的其他方法，如R-CNN方法使用region proposal来生成整张图像中可能包含待检测物体的potential bounding boxes，然后用分类器来评估这些boxes，接着通过post-processing来改善bounding boxes，消除重复的检测目标，并基于整个场景中的其他物体重新对boxes进行打分。整个流程执行下来很慢，而且因为这些环节都是分开训练的，检测性能很难进行优化。
    作者设计了YOLO（You Only Look Once: Unified, Real-Time Object Detection），将物体检测任务当作回归问题（regression problem）来处理，直接通过整张图片的所有像素得到bounding box的坐标、box中包含物体的置信度和class probabilities。通过YOLO，每张图像只需要看一眼就能得出图像中都有哪些物体和这些物体的位置。
     
    图7.4.1
    如图所示，使用YOLO来检测物体，其流程是非常简单明了的：（1）将图像resize到448×448作为神经网络的输入；（2）运行神经网络，得到一些bounding box坐标、box中包含物体的置信度和class probabilities；（3）进行非极大值抑制，筛选Boxes。下图是各物体检测系统的检测流程对比：
     
    图7.4.2
##### （1）Unified Detection 
    YOLO将输入图像划分为S×S的栅格，每个栅格负责检测中心落在该栅格中的物体,如下图所示：
    
    图7.4.3
    每一个栅格预测B个bounding boxes，以及这些bounding boxes的confidence scores。这个confidence scores反映了模型对于这个栅格的预测：该栅格是否含有物体，以及这个box的坐标预测的有多准。如果这个栅格中不存在一个object，则confidence score应该为0；否则的话，confidence score则为predicted bounding box与ground truth box之间的IOU（intersection over union）。YOLO对每个bounding box有5个predictions：x, y, w, h, and confidence。坐标x,y代表了预测的bounding box的中心与栅格边界的相对值。坐标w,h代表了预测的bounding box的width、height相对于整幅图像width,height的比例。confidence就是预测的bounding box和ground truth box的IOU值。
     
    图7.4.4
     
    图7.4.5
    每一个栅格还要预测C个conditional class probability（条件类别概率）：Pr(Classi|Object)。即在一个栅格包含一个Object的前提下，它属于某个类的概率。只为每个栅格预测一组（C个）类概率，而不考虑框B的数量。
     
    图7.4.6
    注意：conditional class probability信息是针对每个网格的。confidence信息是针对每个bounding box的。在测试阶段，将每个栅格的conditional class probabilities与每个bounding box的confidence相乘。这样既可得到每个bounding box的具体类别的confidence score。这乘积既包含了bounding box中预测的class的probability信息，也反映了bounding box是否含有Object和bounding box坐标的准确度。
     
    图7.4.7
    将YOLO用于PASCAL VOC数据集时：论文使用的S=7，即将一张图像分为7×7=49个栅格每一个栅格预测B=2个boxes（每个box有x, y, w, h, confidence，5个预测值），同时C=20（PASCAL数据集中有20个类别）。因此，最后的prediction是7×7×30{即S×S×(B×5+C)}的Tensor。
     
    图7.4.8
     
    图7.4.9
     
    图7.4.10

##### （2）Network Design
    YOLO检测网络包括24个卷积层和2个全连接层,如图所示：
    
    图7.4.11
    
    图7.4.12
    其中，卷积层用来提取图像特征，全连接层用来预测图像位置和类别概率值。YOLO网络借鉴了GoogLeNet分类网络结构。不同的是，YOLO未使用inception module，而是使用1×1卷积层（此处1x1卷积层的存在是为了跨通道信息整合）3×3卷积层进行简单替代。

##### （3）Training
    首先利用ImageNet 1000-class的分类任务数据集Pretrain卷积层。使用上述网络中的前20个卷积层，加上一个average-pooling layer，最后加一个全连接层，作为Pretrain的网络。训练大约一周的时间，使得在ImageNet 2012的验证数据集Top-5的精度达到88%，这个结果跟GoogleNet的效果相当。将Pretrain的结果的前20层卷积层应用到Detection中，并加入剩下的4个卷积层及2个全连接。同时为了获取更精细化的结果，将输入图像的分辨率由224×224提升到448×448。将所有的预测结果都归一化到0-1,使用Leaky RELU作为激活函数。为了防止过拟合，在第一个全连接层后面接了一个ratio=0.5的Dropout层。为了提高精度，对原始图像做数据提升。

#####  （4）损失函数
    损失函数的设计目标就是让坐标（x,y,w,h），confidence，classification这个三个方面达到很好的平衡。简单的全部采用了sum-squared error loss来做这件事会有以下不足：（1）8维的localization error和20维的classification error同等重要显然是不合理的；（2）如果一些栅格中没有object（一幅图中这种栅格很多），那么就会将这些栅格中的bounding box的confidence置为0，相比于较少的有object的栅格，这些不包含物体的栅格对梯度更新的贡献会远大于包含物体的栅格对梯度更新的贡献，这会导致网络不稳定甚至发散。
    
    图7.4.13
    解决方案如下：更重视8维的坐标预测，给这些损失前面赋予更大的loss weight,记为λcoord,在pascal VOC训练中取5（上图蓝色框）。对没有object的bbox的confidence loss，赋予小的loss weight，记为λnoobj，在pascal VOC训练中取0.5（上图橙色框）。有object的bbox的confidence loss(上图红色框)和类别的loss（上图紫色框）的loss weight正常取1。
    对不同大小的bbox预测中，相比于大bbox预测偏一点，小box预测偏相同的尺寸对IOU的影响更大。而sum-square error loss中对同样的偏移loss是一样。为了缓和这个问题，作者用了一个巧妙的办法，就是将box的width和height取平方根代替原本的height和width。在YOLO中，每个栅格预测多个bounding box，但在网络模型的训练中，希望每一个物体最后由一个bounding box predictor来负责预测。因此，当前哪一个predictor预测的bounding box与ground truth box的IOU最大，这个predictor就负责predict object。这会使得每个predictor可以专门的负责特定的物体检测。随着训练的进行，每一个predictor对特定的物体尺寸、长宽比的物体的类别的预测会越来越好。


    图7.4.14神经网络输出后的检测流程
    
    图7.4.15

#####  （5）非极大值抑制

 

    图7.4.16

#####  （6）获取Object Detect结果

    图7.4.17

#####  YOLOv1模型优缺点
#####  （1）YOLO检测物体非常快。
    因为没有复杂的检测流程，只需要将图像输入到神经网络就可以得到检测结果，YOLO可以非常快的完成物体检测任务。标准版本的YOLO在TitanX的GPU上能达到45FPS。更快的Fast YOLO检测速度可以达到155FPS。而且，YOLO的mAP是之前其他实时物体检测系统的两倍以上。

#####  （2）YOLO可以很好的避免背景错误，产生false positives。
    不像其他物体检测系统使用了滑窗或region proposal，分类器只能得到图像的局部信息。YOLO在训练和测试时都能够看到一整张图像的信息，因此YOLO在检测物体时能很好的利用上下文信息，从而不容易在背景上预测出错误的物体信息。和Fast-R-CNN相比，YOLO的背景错误不到Fast-R-CNN的一半。
#####  （3）YOLO可以学到物体的泛化特征。
    当YOLO在自然图像上做训练，在艺术作品上做测试时，YOLO表现的性能比DPM、R-CNN等之前的物体检测系统要好很多。因为YOLO可以学习到高度泛化的特征，从而迁移到其他领域。
    尽管YOLO有这些优点，它也有一些缺点：（1）YOLO的物体检测精度低于其他state-of-the-art的物体检测系统；（2）YOLO容易产生物体的定位错误；（3）YOLO对小物体的检测效果不好（尤其是密集的小物体，因为一个栅格只能预测2个物体）。
    下图是各物体检测系统的检测性能对比：
     
    图7.4.18
####  8.3.2.2 YOLOv2
    YOLOv2相比YOLOv1做了很多方面的改进，这也使得YOLOv2的mAP有显著的提升，并且YOLOv2的速度依然很快，保持着自己作为one-stage方法的优势，YOLOv2和Faster R-CNN,SSD等模型的对比如图7.4.19所示。
    
    图7.4.19 YOLOv2与其它模型在VOC 2007数据集上的效果对比
    YOLOv1虽然检测速度很快，但是在检测精度上却不如R-CNN系检测方法，YOLOv1在物体定位方面（localization）不够准确，并且召回率（recall）较低。YOLOv2共提出了几种改进策略来提升YOLO模型的定位准确度和召回率，从而提高mAP，YOLOv2在改进中遵循一个原则：保持检测速度，这也是YOLO模型的一大优势。YOLOv2的改进策略如图7.4.20所示，可以看出，大部分的改进方法都可以比较显著提升模型的mAP。
#####  YOLOv2改进策略

    图7.4.20 YOLOv2相比YOLOv1的改进策略
#####  （1）Batch Normalization 
    Batch Normalization可以提升模型收敛速度，而且可以起到一定正则化效果，降低模型的过拟合。在YOLOv2中，每个卷积层后面都添加了Batch Normalization层，并且不再使用droput。使用Batch Normalization后，YOLOv2的mAP提升了2.4%。

#####  （2）High Resolution Classifier 
    目前的目标检测方法中，基本上都会使用ImageNet预训练过的模型（classifier）来提取特征，如果用的是AlexNet网络，那么输入图片会被resize到不足256×256，导致分辨率不够高，给检测带来困难。为此，新的YOLO网络把分辨率直接提升到了448×448，这也意味之原有的网络模型必须进行某种调整以适应新的分辨率输入。
    对于YOLOv2，作者首先对分类网络（自定义的darknet）进行了fine tune，分辨率改成448×448，在ImageNet数据集上训练10轮（10epochs），训练后的网络就可以适应高分辨率的输入了。然后，作者对检测网络部分（也就是后半部分）也进行fine tune。这样通过提升输入的分辨率，mAP获得了4%的提升。

#####  （3）Convolutional With Anchor Boxes 
    之前的YOLO利用全连接层的数据完成边框的预测，导致丢失较多的空间信息，定位不准。作者在这一版本中借鉴了Faster R-CNN中的anchor思想，回顾一下，anchor是RNP网络中的一个关键步骤，说的是在卷积特征图上进行滑窗操作，每一个中心可以预测9种不同大小的建议框。
    
    图7.4.21
    为了引入anchor boxes来预测bounding boxes，作者在网络中果断去掉了全连接层。剩下的具体怎么操作呢？
    首先，作者去掉了后面的一个池化层以确保输出的卷积特征图有更高的分辨率。然后，通过缩减网络，让图片输入分辨率为416×416，这一步的目的是为了让后面产生的卷积特征图宽高都为奇数，这样就可以产生一个center cell。作者观察到，大物体通常占据了图像的中间位置，就可以只用中心的一个cell来预测这些物体的位置，否则就要用中间的4个cell来进行预测，这个技巧可稍稍提升效率。最后，YOLOv2使用了卷积层降采样（factor为32），使得输入卷积网络的416×416图片最终得到13×13的卷积特征图（416/32=13）。加入了anchor boxes后，可以预料到的结果是召回率上升，准确率下降。我们来计算一下，假设每个cell预测9个建议框，那么总共会预测13×13×9=1521个boxes，而之前的网络仅仅预测7×7×2=98个boxes。具体数据为：没有anchor boxes，模型recall为81%，mAP为69.5%；加入anchor boxes，模型recall为88%，mAP为69.2%。这样看来，准确率只有小幅度的下降，而召回率则提升了7%，说明可以通过进一步的工作来加强准确率，的确有改进空间。

#####  （4）Dimension Clusters（维度聚类）
    作者在使用anchor的时候遇到了两个问题，第一个是anchor boxes的宽高维度往往是精选的先验框（hand-picked priors），虽说在训练过程中网络也会学习调整boxes的宽高维度，最终得到准确的bounding boxes。但是，如果一开始就选择了更好的、更有代表性的先验boxes维度，那么网络就更容易学到准确的预测位置。和以前的精选boxes维度不同，作者使用了K-means聚类方法类训练bounding boxes，可以自动找到更好的boxes宽高维度。传统的K-means聚类方法使用的是欧氏距离函数，也就意味着较大的boxes会比较小的boxes产生更多的error，聚类结果可能会偏离。为此，作者采用的评判标准是IOU得分（也就是boxes之间的交集除以并集），这样的话，error就和box的尺度无关了，最终的距离函数为：
    
    作者通过改进的K-means对训练集中的boxes进行了聚类，判别标准是平均IOU得分，聚类结果如下图：
    
    图7.4.22数据集VOC和COCO上的边界框聚类分析结果
    可以看到，平衡复杂度和IOU之后，最终得到k值为5，意味着作者选择了5种大小的box维度来进行定位预测，这与手动精选的box维度不同。结果中扁长的框较少，而瘦高的框更多（这符合行人的特征），这种结论如不通过聚类实验恐怕是发现不了的。当然，作者也做了实验来对比两种策略的优劣，如下图，使用聚类方法，仅仅5种boxes的召回率就和Faster R-CNN的9种相当。说明K-means方法的引入使得生成的boxes更具有代表性，为后面的检测任务提供了便利。
    表7.4.1


#####  （5）New Network：Darknet-19
    YOLOv2采用了一个新的基础模型（特征提取器），称为Darknet-19，包括19个卷积层和5个maxpooling层，如图4所示。Darknet-19与VGG16模型设计原则是一致的，主要采用3*3卷积，采用2*2的maxpooling层之后，特征图维度降低2倍，而同时将特征图的channles增加两倍。与NIN(Network in Network)类似，Darknet-19最终采用global avgpooling做预测，并且在3*3卷积之间使用1*1卷积来压缩特征图channles以降低模型计算量和参数。Darknet-19每个卷积层后面同样使用了batch norm层以加快收敛速度，降低模型过拟合。在ImageNet分类数据集上，Darknet-19的top-1准确度为72.9%，top-5准确度为91.2%，但是模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。
    表7.4.2


#####  （6）Direct location prediction 
    前面讲到，YOLOv2借鉴RPN网络使用anchor boxes来预测边界框相对先验框的offsets。边界框的实际中心位置 ，需要根据预测的坐标偏移值 ，先验框的尺度 以及中心坐标 （特征图每个位置的中心点）来计算：
    
    但是上面的公式是无约束的，预测的边界框很容易向任何方向偏移，如当 时边界框将向右偏移先验框的一个宽度大小，而当 时边界框将向左偏移先验框的一个宽度大小，因此每个位置预测的边界框可以落在图片任何位置，这导致模型的不稳定性，在训练时需要很长时间来预测出正确的offsets。所以，YOLOv2弃用了这种预测方式，而是沿用YOLOv1的方法，就是预测边界框中心点相对于对应cell左上角位置的相对偏移值，为了将边界框中心点约束在当前cell中，使用sigmoid函数处理偏移值，这样预测的偏移值在(0,1)范围内（每个cell的尺度看做1）。总结来看，根据边界框预测的4个offsets： ，可以按如下公式计算出边界框实际位置和大小：
    
    其中为 为cell的左上角坐标，如图5所示，在计算时每个cell的尺度为1，所以当前cell的左上角坐标为(1,1)。由于sigmoid函数的处理，边界框的中心位置会约束在当前cell内部，防止偏移过多。而 和 是先验框的宽度与长度，前面说过它们的值也是相对于特征图大小的，在特征图中每个cell的长和宽均为1。这里记特征图的大小为 （在文中是(13,13))，这样我们可以将边界框相对于整张图片的位置和大小计算出来（4个值均在0和1之间）：
    
    如果再将上面的4个值分别乘以图片的宽度和长度（像素点值）就可以得到边界框的最终位置和大小了。这就是YOLOv2边界框的整个解码过程。约束了边界框的位置预测值使得模型更容易稳定训练，结合聚类分析得到先验框与这种预测方法，YOLOv2的mAP值提升了约5%。
    
    图7.4.23边界框位置与大小的计算示例图

#####  （7）Fine-Grained Features
    YOLOv2的输入图片大小为416*416，经过5次maxpooling之后得到13*13大小的特征图，并以此特征图采用卷积做预测。13*13大小的特征图对检测大物体是足够了，但是对于小物体还需要更精细的特征图（Fine-Grained Features）。因此SSD使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。YOLOv2提出了一种passthrough层来利用更精细的特征图。YOLOv2所利用的Fine-Grained Features是26*26大小的特征图（最后一个maxpooling层的输入），对于Darknet-19模型来说就是大小为26*26*512的特征图。passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率特征图上。前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个2*2的局部区域，然后将其转化为channel维度，对于26*26*512的特征图，经passthrough层处理之后就变成了13*13*2048的新特征图（特征图大小降低4倍，而channles增加4倍，图6为一个实例），这样就可以与后面的13*13*1024特征图连接在一起形成13*13*3072的特征图，然后在此特征图基础上卷积做预测。在YOLO的C源码中，passthrough层称为reorg layer。
另外，作者在后期的实现中借鉴了ResNet网络，不是直接对高分辨特征图处理，而是增加了一个中间卷积层，先采用64个1*1卷积核进行卷积，然后再进行passthrough处理，这样26*26*512的特征图得到13*13*256的特征图。这算是实现上的一个小细节。使用Fine-Grained Features之后YOLOv2的性能有1%的提升。

#####  （8）Multi-Scale Training 
    由于YOLOv2模型中只有卷积层和池化层，所以YOLOv2的输入可以不限于416*416大小的图片。为了增强模型的鲁棒性，YOLOv2采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔一定的iterations之后改变模型的输入图片大小。由于YOLOv2的下采样总步长为32，输入图片大小选择一系列为32倍数的值：输入图片最小为320*320，此时对应的特征图大小为10*10（不是奇数了，确实有点尴尬），而输入图片最大为608*608,对应的特征图大小为19*19,在训练过程，每隔10个iterations随机选择一种输入图片大小，然后只需要修改对最后检测层的处理就可以重新训练。
    
    图7.4.24 Multi-Scale Training
    采用Multi-Scale Training策略，YOLOv2可以适应不同大小的图片，并且预测出很好的结果。在测试时，YOLOv2可以采用不同大小的图片作为输入，在VOC 2007数据集上的效果如下图所示。可以看到采用较小分辨率时，YOLOv2的mAP值略低，但是速度更快，而采用高分辨输入时，mAP值更高，但是速度略有下降，对于544*544,mAP高达78.6%。注意，这只是测试时输入图片大小不同，而实际上用的是同一个模型（采用Multi-Scale Training训练）。
     
    图7.4.25 YOLOv2在VOC 2007数据集上的性能对比
    
    总结来看，虽然YOLOv2做了很多改进，但是大部分都是借鉴其它论文的一些技巧，如Faster R-CNN的anchor boxes，YOLOv2采用anchor boxes和卷积做预测，这基本上与SSD模型（单尺度特征图的SSD）非常类似了，而且SSD也是借鉴了Faster R-CNN的RPN网络。从某种意义上来说，YOLOv2和SSD这两个one-stage模型与RPN网络本质上无异，只不过RPN不做类别的预测，只是简单地区分物体与背景。在two-stage方法中，RPN起到的作用是给出region proposals，其实就是作出粗糙的检测，所以另外增加了一个stage，即采用R-CNN网络来进一步提升检测的准确度（包括给出类别预测）。而对于one-stage方法，它们想要一步到位，直接采用“RPN”网络作出精确的预测，要因此要在网络设计上做很多的tricks。YOLOv2的一大创新是采用Multi-Scale Training策略，这样同一个模型其实就可以适应多种大小的图片了。

#####  YOLOv2的训练
    YOLOv2的训练主要包括三个阶段。
    第一阶段：先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为224*224,共训练160个epochs。
    第二阶段：将网络的输入调整为448*448,继续在ImageNet数据集上finetune分类模型，训练10个epochs，此时分类模型的top-1准确度为76.5%，而top-5准确度为93.3%。
    第三个阶段：修改Darknet-19分类模型为检测模型，并在检测数据集上继续finetune网络。
    网络修改包括（网路结构可视化）：移除最后一个卷积层、global avgpooling层以及softmax层，并且新增了三个3*3*2014卷积层，同时增加了一个passthrough层，最后使用1*1卷积层输出预测结果。

####  8.3.2.3 YOLO9000
    YOLO9000是在YOLOv2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。众多周知，检测数据集的标注要比分类数据集打标签繁琐的多，所以ImageNet分类数据集比VOC等检测数据集高出几个数量级。在YOLO中，边界框的预测其实并不依赖于物体的标签，所以YOLO可以实现在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。	作者选择在COCO和ImageNet数据集上进行联合训练，但是遇到的第一问题是两者的类别并不是完全互斥的，比如"Norfolk terrier"明显属于"dog"，所以作者提出了一种层级分类方法（Hierarchical classification），主要思路是根据各个类别之间的从属关系（根据WordNet）建立一种树结构WordTree，结合COCO和ImageNet建立的WordTree如下图所示：
    
    图7.4.26 基于COCO和ImageNet数据集建立的WordTree
    WordTree中的根节点为"physical object"，每个节点的子节点都属于同一子类，可以对它们进行softmax处理。在给出某个类别的预测概率时，需要找到其所在的位置，遍历这个path，然后计算path上各个节点的概率之积。
     
    图7.4.27 ImageNet与WordTree预测的对比
    在训练时，如果是检测样本，按照YOLOv2的loss计算误差，而对于分类样本，只计算分类误差。在预测时，YOLOv2给出的置信度就是Pr(physicalobject)，同时会给出边界框位置以及一个树状概率图。在这个概率图中找到概率最高的路径，当达到某一个阈值时停止，就用当前节点表示预测的类别。通过联合训练策略，YOLO9000可以快速检测出超过9000个类别的物体，总体mAP值为19,7%。我觉得这是作者在这篇论文作出的最大的贡献，因为YOLOv2的改进策略亮点并不是很突出，但是YOLO9000算是开创之举。

####  8.3.2.4 YOLOv3
    YOLOv3在Pascal TitanX上处理608x608图像速度达到20FPS，在COCO test-dev上mAP@0.5达到57.9%，与RetinaNet（FocalLoss论文所提出的单阶段网络）的结果相近，并且速度快4倍。YOLOv3的模型比之前的模型复杂了不少，可以通过改变模型结构的大小来权衡速度与精度。速度对比如下：
    
    图7.4.28 性能对比图
    YOLOv3在实现相同准确度下要显著地比其它检测方法快。时间都是在采用M40或TitanX等相同GPU下测量的。
    简而言之，YOLOv3的先验检测（Prior detection）系统将分类器或定位器重新用于执行检测任务。他们将模型应用于图像的多个位置和尺度。而那些评分较高的区域就可以视为检测结果。此外，相对于其它目标检测方法，我们使用了完全不同的方法。我们将一个单神经网络应用于整张图像，该网络将图像划分为不同的区域，因而预测每一块区域的边界框和概率，这些边界框会通过预测的概率加权。我们的模型相比于基于分类器的系统有一些优势。它在测试时会查看整个图像，所以它的预测利用了图像中的全局信息。与需要数千张单一目标图像的R-CNN不同，它通过单一网络评估进行预测。这令YOLOv3非常快，一般它比R-CNN快1000倍、比Fast R-CNN快100倍。

#####  YOLOv3改进
#####  （1）多尺度预测（类FPN）
    每种尺度预测3个box, anchor的设计方式仍然使用聚类,得到9个聚类中心,将其按照大小均分给3中尺度。
    尺度1:在基础网络之后添加一些卷积层再输出box信息。
    尺度2:从尺度1中的倒数第二层的卷积层上采样(x2)再与最后一个16x16大小的特征图相加,再次通过多个卷积后输出box信息.相比尺度1变大两倍。
    尺度3:与尺度2类似,使用了32x32大小的特征图。

#####  （2）更好的基础分类网络（类ResNet）和分类器darknet-53,见下图。

    图7.4.29 基础网络Darknet-53

#####  （3）分类器-类别预测：
    YOLOv3不使用Softmax对每个框进行分类，主要考虑因素有两个：（1）Softmax使得每个框分配一个类别（score最大的一个），而对于Open Images这种数据集，目标可能有重叠的类别标签，因此Softmax不适用于多标签分类。（2）Softmax可被独立的多个logistic分类器替代，且准确率不会下降。分类损失采用binary cross-entropy loss。仿ResNet, 与ResNet-101或ResNet-152准确率接近,但速度更快.对比如下（准确率（top-1 误差、top-5 误差）、运算次数（/十亿）、每秒浮点数运算次数（/十亿），以及 FPS 值。）:
    表7.4.3主干架构的性能对比
     
    检测结构如下：
     
    图7.4.30
     
    图7.4.31
    从中看出，YOLOv3表现得不错。RetinaNet需要大约3.8倍的时间来处理一张图像，YOLOv3相比SSD变体要好得多，并在AP_50指标上和当前最佳模型有得一拼。YOLOv3在mAP@0.5及小目标APs上具有不错的结果,但随着IOU的增大,性能下降,说明YOLOv3不能很好地与ground truth切合。
    边框预测：
     
    图7.4.32带有维度先验和定位预测的边界框。我们边界框的宽和高以作为离聚类中心的位移，并使用Sigmoid函数预测边界框相对于滤波器应用位置的中心坐标。仍采用之前的logis其中cx,cy是网格的坐标偏移量,pw,ph是预设的anchor box的边长.最终得到的边框坐标值是b*,而网络学习目标是t*，用sigmod函数、指数转换。

#####  YOLOv3模型优缺点
#####  （1）优点
    a)	快速,pipline简单；
    b)	背景误检率低；
    c)	通用性强。
    YOLO对于艺术类作品中的物体检测同样适用。它对非自然图像物体的检测率远远高于DPM和RCNN系列检测方法。

#####  （2）缺点
    但相比RCNN系列物体检测方法，YOLO具有以下缺点：
    a)	识别物体位置精准性差。
    b)	召回率低。
    在每个网格中预测两个box这种约束方式减少了对同一目标的多次检测(R-CNN使用的region proposal方式重叠较多),相比R-CNN使用Selective Search产生2000个proposal（RCNN测试时每张超过40秒），yolo仅使用7x7x2个。

### 8.3.3 RetinaNet

md版本未编辑完
